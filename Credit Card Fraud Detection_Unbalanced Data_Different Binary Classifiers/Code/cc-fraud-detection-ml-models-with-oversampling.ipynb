{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-17T17:08:19.716895Z","iopub.execute_input":"2022-03-17T17:08:19.717239Z","iopub.status.idle":"2022-03-17T17:08:20.781041Z","shell.execute_reply.started":"2022-03-17T17:08:19.717148Z","shell.execute_reply":"2022-03-17T17:08:20.780196Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Load data and review the first 5 rows\ndf = pd.read_csv(\"../input/creditcardfraud/creditcard.csv\")\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:08:22.823882Z","iopub.execute_input":"2022-03-17T17:08:22.824253Z","iopub.status.idle":"2022-03-17T17:08:27.445062Z","shell.execute_reply.started":"2022-03-17T17:08:22.824213Z","shell.execute_reply":"2022-03-17T17:08:27.444108Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# correlation matrix\ncorr = df.corr()\ncorr.head(5)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:09:13.666810Z","iopub.execute_input":"2022-03-17T17:09:13.667359Z","iopub.status.idle":"2022-03-17T17:09:14.488768Z","shell.execute_reply.started":"2022-03-17T17:09:13.667302Z","shell.execute_reply":"2022-03-17T17:09:14.487982Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# correlation of \"Class\", i.e. target with predictors\ncorr['Class'].abs().sort_values(ascending = False).to_frame()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:10:25.179131Z","iopub.execute_input":"2022-03-17T17:10:25.179430Z","iopub.status.idle":"2022-03-17T17:10:25.192776Z","shell.execute_reply.started":"2022-03-17T17:10:25.179399Z","shell.execute_reply":"2022-03-17T17:10:25.191748Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Dimension of data\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:24:26.538163Z","iopub.execute_input":"2022-02-24T22:24:26.539287Z","iopub.status.idle":"2022-02-24T22:24:26.548906Z","shell.execute_reply.started":"2022-02-24T22:24:26.539227Z","shell.execute_reply":"2022-02-24T22:24:26.547523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review the name of columns, their null-values, and their data type\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:10:33.217152Z","iopub.execute_input":"2022-03-17T17:10:33.217874Z","iopub.status.idle":"2022-03-17T17:10:33.258809Z","shell.execute_reply.started":"2022-03-17T17:10:33.217830Z","shell.execute_reply":"2022-03-17T17:10:33.258109Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Review the statistics of columns\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:10:38.473462Z","iopub.execute_input":"2022-03-17T17:10:38.474220Z","iopub.status.idle":"2022-03-17T17:10:38.903065Z","shell.execute_reply.started":"2022-03-17T17:10:38.474185Z","shell.execute_reply":"2022-03-17T17:10:38.902097Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **EDA: **Univariate analysis****","metadata":{}},{"cell_type":"code","source":"# Distribution of Time column \nsns.histplot(df.Time, bins=50)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:10:47.199822Z","iopub.execute_input":"2022-03-17T17:10:47.200088Z","iopub.status.idle":"2022-03-17T17:10:47.745841Z","shell.execute_reply.started":"2022-03-17T17:10:47.200060Z","shell.execute_reply":"2022-03-17T17:10:47.745008Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Since df.Amount is very skewed, we use log(df.Amount+1) for plotting.  ","metadata":{}},{"cell_type":"code","source":"# Distribution of log(Amount) column\nsns.histplot(np.log(df.Amount+1), bins=50)\nplt.xlabel(\"log(Amount)\")","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:11:01.299612Z","iopub.execute_input":"2022-03-17T17:11:01.300295Z","iopub.status.idle":"2022-03-17T17:11:01.835520Z","shell.execute_reply.started":"2022-03-17T17:11:01.300252Z","shell.execute_reply":"2022-03-17T17:11:01.833672Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Distribution of Class column\nclass_counts = df.Class.value_counts()\ng = sns.barplot(x=class_counts.index, y=class_counts.values)\ng.set_xticklabels(['Normal','Fraudulent'])\ng.set_ylabel('counts')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:11:13.190357Z","iopub.execute_input":"2022-03-17T17:11:13.191075Z","iopub.status.idle":"2022-03-17T17:11:13.308039Z","shell.execute_reply.started":"2022-03-17T17:11:13.191030Z","shell.execute_reply":"2022-03-17T17:11:13.307199Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Distribution of Class column\nclass_counts = df.Class.value_counts()\ng = sns.barplot(x=class_counts.index, y=np.log(class_counts.values))\ng.set_xticklabels(['Normal','Fraudulent'])\ng.set_ylabel('log(counts)')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:12:58.704356Z","iopub.execute_input":"2022-03-17T17:12:58.704671Z","iopub.status.idle":"2022-03-17T17:12:58.882724Z","shell.execute_reply.started":"2022-03-17T17:12:58.704638Z","shell.execute_reply":"2022-03-17T17:12:58.882055Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Distribution of Class column\ndf.Class.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:13:09.066951Z","iopub.execute_input":"2022-03-17T17:13:09.067653Z","iopub.status.idle":"2022-03-17T17:13:09.076831Z","shell.execute_reply.started":"2022-03-17T17:13:09.067609Z","shell.execute_reply":"2022-03-17T17:13:09.075907Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Distribution of Class column\ndf.Class.value_counts()\nprint(f\"Percentage of fraudulent transactions is {df.Class.value_counts()[1]/df.shape[0]*100:.3f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:13:11.906266Z","iopub.execute_input":"2022-03-17T17:13:11.906573Z","iopub.status.idle":"2022-03-17T17:13:11.916291Z","shell.execute_reply.started":"2022-03-17T17:13:11.906542Z","shell.execute_reply":"2022-03-17T17:13:11.915387Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"So, the data is highly unbalanced. ","metadata":{}},{"cell_type":"markdown","source":"# **EDA Bivariate Analysis**","metadata":{}},{"cell_type":"code","source":"# Distribution of Amount per Class\ng = sns.catplot(x=\"Class\", y=\"Amount\", data=df)\ng.set_xticklabels(['Normal','Fraudulent'])","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:13:17.813855Z","iopub.execute_input":"2022-03-17T17:13:17.814482Z","iopub.status.idle":"2022-03-17T17:13:21.316983Z","shell.execute_reply.started":"2022-03-17T17:13:17.814429Z","shell.execute_reply":"2022-03-17T17:13:21.315926Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"The range of transaction amount for fraudulent transactions is less than normal ones. ","metadata":{}},{"cell_type":"code","source":"# Distribution of Time per Class\ng = sns.catplot(x=\"Class\", y=\"Time\", data=df)\ng.set_xticklabels(['Normal','Fraudulent'])","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:13:34.969841Z","iopub.execute_input":"2022-03-17T17:13:34.970136Z","iopub.status.idle":"2022-03-17T17:13:38.306296Z","shell.execute_reply.started":"2022-03-17T17:13:34.970102Z","shell.execute_reply":"2022-03-17T17:13:38.305378Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"The range of transaction time is almost the same for both types of transactions. ","metadata":{}},{"cell_type":"code","source":"# Highly correlated columns with Class column (target)\ncorr_df = df.corr()\ncorr_df_class = corr_df[\"Class\"].abs().sort_values(ascending=False)\nprint(corr_df_class[corr_df_class>=0.2].index)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:13:50.842818Z","iopub.execute_input":"2022-03-17T17:13:50.843115Z","iopub.status.idle":"2022-03-17T17:13:51.659861Z","shell.execute_reply.started":"2022-03-17T17:13:50.843085Z","shell.execute_reply":"2022-03-17T17:13:51.658731Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Distribution of mostly correlated columns (with Class column) per Class\nfig, ax = plt.subplots(1, 4, figsize=(15, 5))\n        \nsns.boxplot(x=\"Class\", y=\"V17\", data=df, ax=ax[0])\nsns.boxplot(x=\"Class\", y=\"V14\", data=df, ax=ax[1])\nsns.boxplot(x=\"Class\", y=\"V12\", data=df, ax=ax[2])\nsns.boxplot(x=\"Class\", y=\"V10\", data=df, ax=ax[3])\n","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:14:23.640147Z","iopub.execute_input":"2022-03-17T17:14:23.640570Z","iopub.status.idle":"2022-03-17T17:14:24.528692Z","shell.execute_reply.started":"2022-03-17T17:14:23.640528Z","shell.execute_reply":"2022-03-17T17:14:24.527683Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Distribution of Vi columns per Class\nfig, ax = plt.subplots(7, 4, figsize=(15, 13))\nk=1\nfor i in range(7):\n    for j in range(4):\n        \n        sns.boxplot(x=\"Class\", y=f\"V{k}\", data=df, ax=ax[i, j])\n        k=k+1","metadata":{"execution":{"iopub.status.busy":"2022-02-24T22:46:44.346794Z","iopub.execute_input":"2022-02-24T22:46:44.347076Z","iopub.status.idle":"2022-02-24T22:46:49.804924Z","shell.execute_reply.started":"2022-02-24T22:46:44.347047Z","shell.execute_reply":"2022-02-24T22:46:49.803978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data pre-processing","metadata":{}},{"cell_type":"code","source":"df.duplicated().sum()\n# Checking for duplicated entries\nduplicates = df.duplicated().sum() \nif  duplicates == 0:\n    print(\"There are no duplicted rows in this data\")\nelse:\n    print(f\"There are {duplicates} duplicated rows.\")","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:14:57.000340Z","iopub.execute_input":"2022-03-17T17:14:57.000712Z","iopub.status.idle":"2022-03-17T17:14:59.650208Z","shell.execute_reply.started":"2022-03-17T17:14:57.000677Z","shell.execute_reply":"2022-03-17T17:14:59.649471Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Since duplicates are an extreme case of nonrandom sampling, and they might bias the fitted models, and lead to the model overfitting, we remove them. ","metadata":{}},{"cell_type":"code","source":"# Drop the duplicated rows\ndf=df.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:15:29.439541Z","iopub.execute_input":"2022-03-17T17:15:29.439881Z","iopub.status.idle":"2022-03-17T17:15:30.753273Z","shell.execute_reply.started":"2022-03-17T17:15:29.439843Z","shell.execute_reply":"2022-03-17T17:15:30.751884Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# selecting only required columns for ML models\ndf = df.drop(['Time'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:15:36.032367Z","iopub.execute_input":"2022-03-17T17:15:36.032692Z","iopub.status.idle":"2022-03-17T17:15:36.079511Z","shell.execute_reply.started":"2022-03-17T17:15:36.032659Z","shell.execute_reply":"2022-03-17T17:15:36.078269Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Since the Classes are highly imbalanced, we use oversampling from the minority class, which is adding more copies to the minority class. Oversampling is a good choice here as we don’t have millions of rows to work with, however, it can cause overfitting. \n","metadata":{}},{"cell_type":"code","source":"# Create two different dataframes of majority and minority class \ndf_minority = df[df.Class==1]\ndf_majority = df[df.Class==0]\n\n# Oversample the minority class\nfrom sklearn.utils import resample\ndf_minority_oversampled = resample(df_minority, \n                                 replace=True,    # sample with replacement\n                                 n_samples=len(df_majority) , # to match majority class with len(df_majority) rows\n                                 random_state=42)  # reproducible results\n\n# Combine majority class with oversampled minority class\ndf_oversampled = pd.concat([df_minority_oversampled, df_majority])","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:16:07.491023Z","iopub.execute_input":"2022-03-17T17:16:07.491872Z","iopub.status.idle":"2022-03-17T17:16:07.909858Z","shell.execute_reply.started":"2022-03-17T17:16:07.491815Z","shell.execute_reply":"2022-03-17T17:16:07.908379Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df_oversampled.Class.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:16:24.342267Z","iopub.execute_input":"2022-03-17T17:16:24.342606Z","iopub.status.idle":"2022-03-17T17:16:24.357166Z","shell.execute_reply.started":"2022-03-17T17:16:24.342573Z","shell.execute_reply":"2022-03-17T17:16:24.355947Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"g = sns.countplot(df_oversampled.Class)\ng.set_xticklabels(['Normal','Fraudulent'])","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:16:27.633039Z","iopub.execute_input":"2022-03-17T17:16:27.633732Z","iopub.status.idle":"2022-03-17T17:16:27.914022Z","shell.execute_reply.started":"2022-03-17T17:16:27.633681Z","shell.execute_reply":"2022-03-17T17:16:27.913075Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Now, we have a balanced data. ","metadata":{}},{"cell_type":"code","source":"# Target y and features X for developing ML models\ny = df_oversampled.Class\nX = df_oversampled.drop(['Class'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:16:41.704941Z","iopub.execute_input":"2022-03-17T17:16:41.705524Z","iopub.status.idle":"2022-03-17T17:16:41.782642Z","shell.execute_reply.started":"2022-03-17T17:16:41.705473Z","shell.execute_reply":"2022-03-17T17:16:41.781444Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# split the data (X, y) to train-data and test-data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.3, stratify=y, random_state=101)\n\nfrom sklearn.metrics import recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:16:51.805501Z","iopub.execute_input":"2022-03-17T17:16:51.805790Z","iopub.status.idle":"2022-03-17T17:16:52.372093Z","shell.execute_reply.started":"2022-03-17T17:16:51.805760Z","shell.execute_reply":"2022-03-17T17:16:52.370938Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Any ML algorithm that computes the distance between the data points requires feature scaling (Standardization or Normalization), such as Logistic Regression, and SVM (Support Vector Machine). However, the ML algorithms which are tree-based do not need feature scaling , such as Random Forests, and\nGradient Boosted Decision Trees.\n","metadata":{}},{"cell_type":"code","source":"# Standardize features by removing the mean and scaling to unit variance.\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nss.fit(X_train)\nX_train_ss = ss.transform(X_train)\nX_test_ss = ss.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:17:29.460224Z","iopub.execute_input":"2022-03-17T17:17:29.460545Z","iopub.status.idle":"2022-03-17T17:17:29.786672Z","shell.execute_reply.started":"2022-03-17T17:17:29.460512Z","shell.execute_reply":"2022-03-17T17:17:29.785483Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# **ML Models**","metadata":{}},{"cell_type":"code","source":"# A function to store the performance of each classifier model\ndef ML_models_performance(model, X_train, y_train ,X_test ,y_test, y_pred, test_probs, model_name):\n \n    performance_df=pd.DataFrame({'Train_accuracy':model.score(X_train,y_train),\"Test_accuracy\":model.score(X_test,y_test),\n                       \"Precision\":precision_score(y_pred,y_test),\"Recall\":recall_score(y_pred,y_test),\n                       \"F1_Score\":f1_score(y_pred,y_test), \"roc_auc_score\":roc_auc_score(y_test, test_probs[:, 1])}, index=[model_name])\n    return performance_df","metadata":{"execution":{"iopub.status.busy":"2022-03-17T17:17:39.293140Z","iopub.execute_input":"2022-03-17T17:17:39.293954Z","iopub.status.idle":"2022-03-17T17:17:39.300711Z","shell.execute_reply.started":"2022-03-17T17:17:39.293913Z","shell.execute_reply":"2022-03-17T17:17:39.299970Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n# Create the LogisticRegression model\nlr = LogisticRegression(solver = 'sag', random_state=1)\n# Fit the model\nlr.fit(X_train_ss, y_train)\n# Use the trained model to predict Classes\ny_pred_lr = lr.predict(X_test_ss)\n# Use the trained model to calculate the probabilities of each Class\ntest_probs_lr = lr.predict_proba(X_test_ss)\n\n# model performance\nprint(f'Model train accuracy: {lr.score(X_train_ss, y_train)*100:.3f}%')\nprint(f'Model test accuracy: {lr.score(X_test_ss, y_test)*100:.3f}%')\nprint(f'Model test precision: {precision_score(y_pred_lr,y_test):.3f}')\nprint(f'Model test recall: {recall_score(y_pred_lr,y_test):.3f}')\nprint(f'Model test f1_score: {f1_score(y_pred_lr,y_test):.3f}')\nprint(f'Model test roc_auc_score: {roc_auc_score(y_test, test_probs_lr[:, 1]):.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:47:57.622195Z","iopub.execute_input":"2022-02-24T00:47:57.622779Z","iopub.status.idle":"2022-02-24T00:48:24.448297Z","shell.execute_reply.started":"2022-02-24T00:47:57.622728Z","shell.execute_reply":"2022-02-24T00:48:24.446791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_performance = ML_models_performance(lr, X_train_ss, y_train ,X_test_ss ,y_test, y_pred_lr, test_probs_lr, \"Logisitc Regression\")\nlr_performance","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:48:38.239444Z","iopub.execute_input":"2022-02-24T00:48:38.240206Z","iopub.status.idle":"2022-02-24T00:48:38.662231Z","shell.execute_reply.started":"2022-02-24T00:48:38.240148Z","shell.execute_reply":"2022-02-24T00:48:38.661104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n# Create the RandomForestClassifier model\nrf = RandomForestClassifier(random_state=1)\n# Fit the model\nrf.fit(X_train_ss, y_train)\n# Use the trained model to predict Classes\ny_pred_rf = rf.predict(X_test_ss)\n# Use the trained model to calculate the probabilities of each Class\ntest_probs_rf = rf.predict_proba(X_test_ss)\n\n# model performance\nprint(f'Model train accuracy: {rf.score(X_train_ss, y_train)*100:.3f}%')\nprint(f'Model test accuracy: {rf.score(X_test_ss, y_test)*100:.3f}%')\nprint(f'Model test precision: {precision_score(y_pred_rf,y_test):.3f}')\nprint(f'Model test recall: {recall_score(y_pred_rf,y_test):.3f}')\nprint(f'Model test f1_score: {f1_score(y_pred_rf,y_test):.3f}')\nprint(f'Model test roc_auc_score: {roc_auc_score(y_test, test_probs_rf[:, 1]):.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:48:42.004926Z","iopub.execute_input":"2022-02-24T00:48:42.005243Z","iopub.status.idle":"2022-02-24T00:52:26.452926Z","shell.execute_reply.started":"2022-02-24T00:48:42.005205Z","shell.execute_reply":"2022-02-24T00:52:26.451785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_performance = ML_models_performance(rf, X_train_ss, y_train ,X_test_ss ,y_test, y_pred_rf, test_probs_rf, \"Random Forest\")\nrf_performance","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:52:42.13144Z","iopub.execute_input":"2022-02-24T00:52:42.132233Z","iopub.status.idle":"2022-02-24T00:52:48.827063Z","shell.execute_reply.started":"2022-02-24T00:52:42.13218Z","shell.execute_reply":"2022-02-24T00:52:48.825954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.calibration import CalibratedClassifierCV\n# Create the LinearSVC model\nlsvc = LinearSVC(random_state=1, dual=False)\n# Fit the model\nlsvc.fit(X_train_ss, y_train)\n# Use the trained model to predict Classes\ny_pred_lsvc = lsvc.predict(X_test_ss)\n# AS LinearSVC does not generate probabilities directly we use CalibratedClassifierCV to calculate the probabilities of each Class\nclf = CalibratedClassifierCV(lsvc) \nclf.fit(X_train_ss, y_train)\ntest_probs_lsvc = clf.predict_proba(X_test_ss)\n\n# model performance\nprint(f'Model train accuracy: {lsvc.score(X_train_ss, y_train)*100:.3f}%')\nprint(f'Model test accuracy: {lsvc.score(X_test_ss, y_test)*100:.3f}%')\nprint(f'Model test precision: {precision_score(y_pred_lsvc,y_test):.3f}')\nprint(f'Model test recall: {recall_score(y_pred_lsvc,y_test):.3f}')\nprint(f'Model test f1_score: {f1_score(y_pred_lsvc,y_test):.3f}')\nprint(f'Model test roc_auc_score: {roc_auc_score(y_test, test_probs_lsvc[:, 1]):.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:52:51.912081Z","iopub.execute_input":"2022-02-24T00:52:51.912382Z","iopub.status.idle":"2022-02-24T00:53:25.114157Z","shell.execute_reply.started":"2022-02-24T00:52:51.912351Z","shell.execute_reply":"2022-02-24T00:53:25.113288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lsvc_performance = ML_models_performance(lsvc, X_train_ss, y_train ,X_test_ss ,y_test, y_pred_lsvc, test_probs_lsvc, \"LinearSVC\")\nlsvc_performance","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:55:35.30775Z","iopub.execute_input":"2022-02-24T00:55:35.308535Z","iopub.status.idle":"2022-02-24T00:55:35.728141Z","shell.execute_reply.started":"2022-02-24T00:55:35.308483Z","shell.execute_reply":"2022-02-24T00:55:35.727143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.calibration import CalibratedClassifierCV\n# Create the SVC model with ’rbf’ kernel\nsvcg = SVC(random_state=1)\n# Fit the model\nsvcg.fit(X_train_ss, y_train)\n# Use the trained model to predict Classes\ny_pred_svcg = svcg.predict(X_test_ss)\n# we use CalibratedClassifierCV to calculate the probabilities of each Class\nclf = CalibratedClassifierCV(svcg) \nclf.fit(X_train_ss, y_train)\ntest_probs_svcg = clf.predict_proba(X_test_ss)\n\n# model performance\nprint(f'Model train accuracy: {svcg.score(X_train_ss, y_train)*100:.3f}%')\nprint(f'Model test accuracy: {svcg.score(X_test_ss, y_test)*100:.3f}%')\nprint(f'Model test precision: {precision_score(y_pred_svcg,y_test):.3f}')\nprint(f'Model test recall: {recall_score(y_pred_svcg,y_test):.3f}')\nprint(f'Model test f1_score: {f1_score(y_pred_svcg,y_test):.3f}')\nprint(f'Model test roc_auc_score: {roc_auc_score(y_test, test_probs_svcg[:, 1]):.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:17:40.031797Z","iopub.execute_input":"2022-02-24T00:17:40.032465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svcg_performance = ML_models_performance(svcg, X_train_ss, y_train ,X_test_ss ,y_test, y_pred_svcg, test_probs_svcg, \"SVC\")\nsvcg_performance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\n# Create the XGBClassifier model \nxgb=XGBClassifier(random_state=1)\n# Fit the model\nxgb.fit(X_train_ss,y_train)\n# Use the trained model to predict\ny_pred_xgb = xgb.predict(X_test_ss)\ntest_probs_xgb = xgb.predict_proba(X_test_ss)\n\n# model performance\nprint(f'Model train accuracy: {xgb.score(X_train_ss, y_train)*100:.3f}%')\nprint(f'Model test accuracy: {xgb.score(X_test_ss, y_test)*100:.3f}%')\nprint(f'Model test precision: {precision_score(y_pred_xgb,y_test):.3f}')\nprint(f'Model test recall: {recall_score(y_pred_xgb,y_test):.3f}')\nprint(f'Model test f1_score: {f1_score(y_pred_xgb,y_test):.3f}')\nprint(f'Model test roc_auc_score: {roc_auc_score(y_test, test_probs_xgb[:, 1]):.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:45:44.627218Z","iopub.execute_input":"2022-02-24T00:45:44.627832Z","iopub.status.idle":"2022-02-24T00:47:12.050076Z","shell.execute_reply.started":"2022-02-24T00:45:44.62778Z","shell.execute_reply":"2022-02-24T00:47:12.048948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_performance = ML_models_performance(xgb, X_train_ss, y_train ,X_test_ss ,y_test, y_pred_xgb, test_probs_xgb, \"XGBClassifier\")\nxgb_performance","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:55:43.604537Z","iopub.execute_input":"2022-02-24T00:55:43.604879Z","iopub.status.idle":"2022-02-24T00:55:44.669605Z","shell.execute_reply.started":"2022-02-24T00:55:43.604843Z","shell.execute_reply":"2022-02-24T00:55:44.668739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compare the performance of different models\ncomparison_df = pd.concat([lr_performance, rf_performance, lsvc_performance, xgb_performance])\ncomparison_df","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:55:47.237012Z","iopub.execute_input":"2022-02-24T00:55:47.237298Z","iopub.status.idle":"2022-02-24T00:55:47.253492Z","shell.execute_reply.started":"2022-02-24T00:55:47.237267Z","shell.execute_reply":"2022-02-24T00:55:47.252613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that ROC curves are used when there are roughly equal numbers of observations for each class, while\nPrecision-Recall curves are used when there is class imbalance. Note that ROC curves show an optimistic performance of the ML model on datasets with a class imbalance. The reason for this is because of the use of true negatives in the False Positive Rate (x-axis) in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.","metadata":{}},{"cell_type":"code","source":"# Plot roc curves for different classifier models\nlr_fpr, lr_tpr, _ = roc_curve(y_test, test_probs_lr[:, 1])\nrf_fpr, rf_tpr, _ = roc_curve(y_test, test_probs_rf[:, 1])\nlsvc_fpr, lsvc_tpr, _ = roc_curve(y_test, test_probs_lsvc[:, 1])\n#svcg_fpr, svcg_tpr, _ = roc_curve(y_test, test_probs_svcg[:, 1])\nxgb_fpr, xgb_tpr, _ = roc_curve(y_test, test_probs_xgb[:, 1])\n\n\n# plot the roc curve for different classifier models\nplt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic REgression')\nplt.plot(rf_fpr, rf_tpr, marker='o', label='Random Forest')\nplt.plot(lsvc_fpr, lsvc_tpr, marker='v', label='LinearSVC')\n#plt.plot(svcg_fpr, svcg_tpr, marker='^', label='SVC')\nplt.plot(xgb_fpr, xgb_tpr, marker='*', label='XGBClassifier')\n\n# plot the roc curve for no-skill model\n# generate a no skill prediction (majority class)\nns_probs = [0 for i in range(len(y_test))]\n# calculate roc_auc_score\nns_auc = roc_auc_score(y_test, ns_probs)\n# calculate roc curve\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T01:04:10.822258Z","iopub.execute_input":"2022-02-24T01:04:10.822584Z","iopub.status.idle":"2022-02-24T01:04:11.254338Z","shell.execute_reply.started":"2022-02-24T01:04:10.822544Z","shell.execute_reply":"2022-02-24T01:04:11.253265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve \n\n# Plot precision_recall curve for different classifier models\nlr_precision, lr_recall, _ = precision_recall_curve(y_test, test_probs_lr[:, 1])\nrf_precision, rf_recall, _ = precision_recall_curve(y_test, test_probs_rf[:, 1])\nlsvc_precision, lsvc_recall, _ = precision_recall_curve(y_test, test_probs_lsvc[:, 1])\n#svcg_precision, svcg_recall, _ = precision_recall_curve(y_test, test_probs_svcg[:, 1])\nxgb_precision, xgb_recall, _ = precision_recall_curve(y_test, test_probs_xgb[:, 1])\n\n\n# plot the precision_recall curve for different classifier models\nplt.plot(lr_recall, lr_precision, marker='.', label='Logistic REgression')\nplt.plot(rf_recall, rf_precision, marker='o', label='Random Forest')\nplt.plot(lsvc_recall, lsvc_precision, marker='v', label='LinearSVC')\n#plt.plot(svcg_recall, svcg_precision, marker='^', label='SVC')\nplt.plot(xgb_recall, xgb_precision, marker='*', label='XGBClassifier')\n\n# plot the precision_recall curves for no-skill model\nno_skill = len(y_test[y_test==1]) / len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n\n# axis labels\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T01:00:25.907424Z","iopub.execute_input":"2022-02-24T01:00:25.90822Z","iopub.status.idle":"2022-02-24T01:00:26.70142Z","shell.execute_reply.started":"2022-02-24T01:00:25.908174Z","shell.execute_reply":"2022-02-24T01:00:26.700417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, based on the comparison_df table, the roc_curve and the precision_recall_curve, XGBClassifier and Random Forest are considered good classifier models for this dataset.\n\nIn general, tree-based algorithms usually perform well on imbalanced data, as they work by learning a hierarchy of if/else questions, which make both classes be addressed. ","metadata":{}}]}